{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Minimum Viable Graph (MVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Data import steps:\n",
    "\n",
    "1. iteratively load all json files that we had cleaned up from the source Form 10k\n",
    "2. from each json, extract metadata and the text sections (there are multiple sections, named \"item1\", \"item1a\", \"item7\" and \"item7a\".)\n",
    "3. for each section, split the text up into chunks and calculate a vector embedding for the text in that chunk\n",
    "4. for each chunk create a node in the knowledge graph with the metadata from the form, the text and the text embedding\n",
    "\n",
    "---\n",
    "\n",
    "So, you have access to a json file with a Form 10k.\n",
    "\n",
    "\n",
    "The resulting nodes in the knowledge graph will have the following schema:\n",
    "```cypher\n",
    "(:Chunk \n",
    "  chunkId: string\n",
    "  chunkSeqId: int\n",
    "  cik: int\n",
    "  cusip6: string\n",
    "  f10kItem: string\n",
    "  source: string\n",
    "  text: string\n",
    "  textEmbedding: [float]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages\n",
    "\n",
    "To start we'll load some useful python packages,\n",
    "including some great stuff from langchain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Common data processing\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Set up Neo4j and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gglobal variables\n",
    "\n",
    "You will set up some global variable from the environment and some constants that\n",
    "to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load from environment\n",
    "load_dotenv('.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "DATA_DIR = f\"../{os.getenv('DATA_DIR') or 'data/single'}\"\n",
    "\n",
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'form_10k_chunks'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'textEmbedding'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Neo4j at neo4j://localhost:7687 as neo4j\n",
      "Using data from ../data/sample\n"
     ]
    }
   ],
   "source": [
    "print(f\"Connecting to Neo4j at {NEO4J_URI} as {NEO4J_USERNAME}\")\n",
    "print(f\"Using data from {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a knowledge graph interface\n",
    "\n",
    "You can use the Langchain `Neo4jGraph` interface to send queries\n",
    "to the Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a knowledge graph using Langchain's Neo4j integration.\n",
    "# This will be used for direct querying of the knowledge graph. \n",
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form 10k pre-preprocessing\n",
    "\n",
    "The Form10k data you will be working with has been preprocessed from the original source. \n",
    "\n",
    "The Form 10K forms are complicated files.\n",
    "\n",
    "The pre-processing extracted useful text sections,\n",
    "then combined that with a mapping file \n",
    "that has extra metadata about the filing company.\n",
    "\n",
    "The preprocessing steps included:\n",
    "- downloading the original Form 10k files from the SEC website\n",
    "- extracting sections of text from the forms\n",
    "- storing the extracted text along with some metadata in a json file\n",
    "  - \"source\" - each json retains a URL for the original download source\"\n",
    "    For example:\n",
    "    ```\n",
    "    \"https://www.sec.gov/Archives/edgar/data/106040/000010604023000024/0000106040-23-000024-index.htm\"\n",
    "    ```\n",
    "- the metadata from the mapping file includes:\n",
    "  - \"cik\": a \"central index key\" for the company, defined by the SEC\n",
    "  - \"cusip6\": a cusip code identifies a financial instrumenet, like a stock,\n",
    "    issued by a particular company.\n",
    "    The first 6 characters of the \"cusip\" code are used to identify the company.\n",
    "    defined by Committee on Uniform Securities Identification Procedures.\n",
    "  - \"cusip\": the full 9-character cusip code extends the base cusip with specific\n",
    "    information about the financial instrument itself. \n",
    "    A single company will a single 6-character cusip code, but may have many\n",
    "    9-character cusip codes for different financial instruments.\n",
    "    For example: \n",
    "    ```\n",
    "    [\n",
    "        \"958102905\",\n",
    "        \"958102955\",\n",
    "        \"958102105\",\n",
    "        \"958102AM7\"\n",
    "    ]\n",
    "    ```\n",
    "  - \"names\": an array of known names for the company. \n",
    "    For example:\n",
    "    ```\n",
    "      [\n",
    "        \"WESTERN DIGITAL CORP\",\n",
    "        \"WESTERN DIGITAL CORP.\"\n",
    "    ]\n",
    "    ```\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step inspection of a single form 10k document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with one file\n",
    "\n",
    "Get the the file name and then loading the json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sample/form10k/0001650372-23-000040.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file_name = [f\"{DATA_DIR}/form10k/\" + x for x in os.listdir(f\"{DATA_DIR}/form10k/\")][0]\n",
    "\n",
    "first_file_as_object = json.load(open(first_file_name))\n",
    "\n",
    "first_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - look at the available keys\n",
    "\n",
    "You can loop through they keys to check what the json contains.\n",
    "\n",
    "The first few keys, with names like \"item1\" are the text extracted from the form 10k.\n",
    "The names match the sections within the form.\n",
    "\n",
    "Then there are some fields that were pulled from the mapping file,\n",
    "including cik, cusip6, cusip and names.\n",
    "\n",
    "The source field contains a URL to the download page on the SEC website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item1 <class 'str'>\n",
      "item1a <class 'str'>\n",
      "item7 <class 'str'>\n",
      "item7a <class 'str'>\n",
      "cik <class 'str'>\n",
      "cusip6 <class 'str'>\n",
      "cusip <class 'list'>\n",
      "names <class 'list'>\n",
      "source <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for k,v in first_file_as_object.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - look at one of the items\n",
    "\n",
    "Take a look at one of the items to see what the text is like.\n",
    "\n",
    "These sections can be quite long. So you can use a text splitter \n",
    "to break the text into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>ITEM\\xa01. BUSINESS\\nCompany Overview\\n\\xa0\\nOur mission is to unleash the potential of every team.\\nOur products help teams organize, discuss and complete shared work — delivering superior outcomes for their organizations.\\nOur primary products include Jira Software and Jira Work Management for planning and project management, Confluence for content creation and sharing, Trello for capturing and adding structure to fluid, fast-forming work for teams, Jira Service Management for team service, management and support applications, Jira Align for enterprise agile planning, and Bitbucket for code sharing and management. Together, our products form an integrated system for organizing, discussing and completing shared work, becoming deeply entrenched in how teams collaborate and how organizations run. The Atlassian platform is the common technology foundation for our products that drives connection between teams, information, and workflows. It allows work to flow seamlessly across tools, automates the mundane so teams can focus on what matters, and enables better decision-making based on the data customers choose to put into our products.\\nOur products serve teams of all shapes and sizes, in virtually every industry. Our pricing strategy is unique within the enterprise software industry because we transparently share our affordable pricing online for most of our products and we generally do not follow the practice of opaque pricing and ad hoc discounting. By delivering high-value, low cost pr'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1_text = first_file_as_object['item1']\n",
    "\n",
    "item1_text[0:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - text splitter from Langchain\n",
    "\n",
    "You can use a text splitter function from Langchain.\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` will use newlines\n",
    "and then whitespace characters to break down a text until\n",
    "the chunks are small enough. This strategy is generally\n",
    "good at keeping paragraphs together.\n",
    "\n",
    "Set a chunk size of 2000 characters,\n",
    "with 200 characters of overlap between each chunk,\n",
    "using the built-in `len` function to calculate the \n",
    "text length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into chunks using the RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - text splitter demonstration\n",
    "\n",
    "You can see what the text splitter will do by splitting up\n",
    "the text from \"item1\" that we saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "item1_text_chunks = text_splitter.split_text(item1_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - what's in the chunk\n",
    "\n",
    "As expected, the first chunk looks like the beginning of the\n",
    "text you'd seen earlier.\n",
    "\n",
    "It's always worth doing some sanity checking along the way\n",
    "when doing data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>ITEM\\xa01. BUSINESS\\nCompany Overview\\n\\xa0\\nOur mission is to unleash the potential of every team.\\nOur products help teams organize, discuss and complete shared work — delivering superior outcomes for their organizations.\\nOur primary products include Jira Software and Jira Work Management for planning and project management, Confluence for content creation and sharing, Trello for capturing and adding structure to fluid, fast-forming work for teams, Jira Service Management for team service, management and support applications, Jira Align for enterprise agile planning, and Bitbucket for code sharing and management. Together, our products form an integrated system for organizing, discussing and completing shared work, becoming deeply entrenched in how teams collaborate and how organizations run. The Atlassian platform is the common technology foundation for our products that drives connection between teams, information, and workflows. It allows work to flow seamlessly across tools, automates the mundane so teams can focus on what matters, and enables better decision-making based on the data customers choose to put into our products.\\nOur products serve teams of all shapes and sizes, in virtually every industry. Our pricing strategy is unique within the enterprise software industry because we transparently share our affordable pricing online for most of our products and we generally do not follow the practice of opaque pricing and ad hoc discounting. By delivering high-value, low cost products in pursuit of customer volume, and targeting every organization, regardless of size, industry, or geography we are able to operate at unusual scale for an enterprise software company, with more than 260,000 customers across virtually every industry sector in approximately 200 countries as of June\\xa030, 2023.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1_text_chunks[0]\n",
    "\n",
    "# edit - wrap this line in len() function to get next cell\n",
    "# len(item1_text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - helper function for loading form 10k data\n",
    "\n",
    "You can now create a helper function to load the form 10k data from the json files.\n",
    "\n",
    "This function will load the file as a json object, \n",
    "\n",
    "then for each text section in the json, it will split the text into chunks\n",
    "\n",
    "for each chunk, it will create a new object with the chunk text and the metadata from the form 10k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def split_form10k_data_from_file(file):\n",
    "    chunks_with_metadata = [] # use this to accumlate chunk records\n",
    "    file_as_object = json.load(open(file)) # open the json file\n",
    "    for item in ['item1','item1a','item7','item7a']: # pull these keys from the json\n",
    "        print(f'Processing {item} from {file}') \n",
    "        item_text = file_as_object[item] # grab the text of the item\n",
    "        item_text_chunks = text_splitter.split_text(item_text) # split the text into chunks\n",
    "        chunk_seq_id = 0\n",
    "        for chunk in item_text_chunks[:20]: # only take the first 20 chunks\n",
    "            form_id = file[file.rindex('/') + 1:file.rindex('.')] # extract form id from file name\n",
    "            # finally, construct a record with metadata and the chunk text\n",
    "            chunks_with_metadata.append({\n",
    "                'text': chunk, \n",
    "                # metadata from looping...\n",
    "                'f10kItem': item,\n",
    "                'chunkSeqId': chunk_seq_id,\n",
    "                # constructed metadata...\n",
    "                'formId': f'{form_id}', # pulled from the filename\n",
    "                'chunkId': f'{form_id}-{item}-chunk{chunk_seq_id:04d}',\n",
    "                # metadata from file...\n",
    "                'names': file_as_object['names'],\n",
    "                'cik': file_as_object['cik'],\n",
    "                'cusip6': file_as_object['cusip6'],\n",
    "                'source': file_as_object['source'],\n",
    "            })\n",
    "            chunk_seq_id += 1\n",
    "        print(f'\\tSplit into {chunk_seq_id} chunks')\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sample/form10k/0001650372-23-000040.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item1 from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 4 chunks\n"
     ]
    }
   ],
   "source": [
    "first_file_chunks = split_form10k_data_from_file(first_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - take a look at the first chunk\n",
    "\n",
    "You can take a look at the first chunk record to see what it looks like.\n",
    "\n",
    "As you'd expect, it has the metadata from the form 10k, the text from the chunk, and the calculated properties like\n",
    "`formId` and `chunkId`.\n",
    "\n",
    "Notice that this is from a company named \"Netapp\". Later you can ask questions about this company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '>ITEM\\xa01. BUSINESS\\nCompany Overview\\n\\xa0\\nOur mission is to unleash the potential of every team.\\nOur products help teams organize, discuss and complete shared work — delivering superior outcomes for their organizations.\\nOur primary products include Jira Software and Jira Work Management for planning and project management, Confluence for content creation and sharing, Trello for capturing and adding structure to fluid, fast-forming work for teams, Jira Service Management for team service, management and support applications, Jira Align for enterprise agile planning, and Bitbucket for code sharing and management. Together, our products form an integrated system for organizing, discussing and completing shared work, becoming deeply entrenched in how teams collaborate and how organizations run. The Atlassian platform is the common technology foundation for our products that drives connection between teams, information, and workflows. It allows work to flow seamlessly across tools, automates the mundane so teams can focus on what matters, and enables better decision-making based on the data customers choose to put into our products.\\nOur products serve teams of all shapes and sizes, in virtually every industry. Our pricing strategy is unique within the enterprise software industry because we transparently share our affordable pricing online for most of our products and we generally do not follow the practice of opaque pricing and ad hoc discounting. By delivering high-value, low cost products in pursuit of customer volume, and targeting every organization, regardless of size, industry, or geography we are able to operate at unusual scale for an enterprise software company, with more than 260,000 customers across virtually every industry sector in approximately 200 countries as of June\\xa030, 2023.',\n",
       " 'f10kItem': 'item1',\n",
       " 'chunkSeqId': 0,\n",
       " 'formId': '0001650372-23-000040',\n",
       " 'chunkId': '0001650372-23-000040-item1-chunk0000',\n",
       " 'names': ['ATLASSIAN CORP PLC', 'ATLASSIAN CORPORATION PLC'],\n",
       " 'cik': '1650372',\n",
       " 'cusip6': 'G06242',\n",
       " 'source': 'https://www.sec.gov/Archives/edgar/data/1650372/000165037223000040/0001650372-23-000040-index.htm'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a graph from the chunks\n",
    "\n",
    "### Script - create a graph from the chunks\n",
    "\n",
    "You now have chunks prepared for creating a knowledge graph.\n",
    "\n",
    "The graph will have 1 node per chunk, containing the chunk text and metadata as properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - merge chunk query\n",
    "\n",
    "You will use a Cypher query to merge the chunks into the graph.\n",
    "\n",
    "This query accepts a query parameter called `chunkParam` which is expected\n",
    "to have the data record containing the chunk and metadata.\n",
    "\n",
    "The `MERGE` query will first match an existing node with the same `chunkId` property.\n",
    "\n",
    "If no such node exists, it will create a new node and the `ON CREATE` clause will set the properties using values from the `chunkParam` query parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chunk_node_query = \"\"\"\n",
    "MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        mergedChunk.names = $chunkParam.names,\n",
    "        mergedChunk.formId = $chunkParam.formId, \n",
    "        mergedChunk.cik = $chunkParam.cik, \n",
    "        mergedChunk.cusip6 = $chunkParam.cusip6, \n",
    "        mergedChunk.source = $chunkParam.source, \n",
    "        mergedChunk.f10kItem = $chunkParam.f10kItem, \n",
    "        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "        mergedChunk.text = $chunkParam.text\n",
    "RETURN mergedChunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - create 1 chunk\n",
    "\n",
    "Try creating a node using that query by passing in the first\n",
    "chunk as a parameter.\n",
    "\n",
    "Thre returned value is the newly created node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mergedChunk': {'formId': '0001650372-23-000040',\n",
       "   'f10kItem': 'item1',\n",
       "   'names': ['ATLASSIAN CORP PLC', 'ATLASSIAN CORPORATION PLC'],\n",
       "   'cik': '1650372',\n",
       "   'cusip6': 'G06242',\n",
       "   'source': 'https://www.sec.gov/Archives/edgar/data/1650372/000165037223000040/0001650372-23-000040-index.htm',\n",
       "   'text': '>ITEM\\xa01. BUSINESS\\nCompany Overview\\n\\xa0\\nOur mission is to unleash the potential of every team.\\nOur products help teams organize, discuss and complete shared work — delivering superior outcomes for their organizations.\\nOur primary products include Jira Software and Jira Work Management for planning and project management, Confluence for content creation and sharing, Trello for capturing and adding structure to fluid, fast-forming work for teams, Jira Service Management for team service, management and support applications, Jira Align for enterprise agile planning, and Bitbucket for code sharing and management. Together, our products form an integrated system for organizing, discussing and completing shared work, becoming deeply entrenched in how teams collaborate and how organizations run. The Atlassian platform is the common technology foundation for our products that drives connection between teams, information, and workflows. It allows work to flow seamlessly across tools, automates the mundane so teams can focus on what matters, and enables better decision-making based on the data customers choose to put into our products.\\nOur products serve teams of all shapes and sizes, in virtually every industry. Our pricing strategy is unique within the enterprise software industry because we transparently share our affordable pricing online for most of our products and we generally do not follow the practice of opaque pricing and ad hoc discounting. By delivering high-value, low cost products in pursuit of customer volume, and targeting every organization, regardless of size, industry, or geography we are able to operate at unusual scale for an enterprise software company, with more than 260,000 customers across virtually every industry sector in approximately 200 countries as of June\\xa030, 2023.',\n",
       "   'chunkId': '0001650372-23-000040-item1-chunk0000',\n",
       "   'chunkSeqId': 0}}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(merge_chunk_node_query, \n",
    "         params={'chunkParam':first_file_chunks[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - prepare unique constraint\n",
    "\n",
    "Before calling the helper function to create a knowledge graph,\n",
    "we will take one extra step to make sure we don't duplicate data.\n",
    "\n",
    "In the previous lesson, you created a vector index.\n",
    "\n",
    "The uniqueness constraint is also index. It's job is to ensure that\n",
    "a particular property is unique for all nodes that share a common label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 7, 'name': 'unique_chunk', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'RANGE', 'entityType': 'NODE', 'labelsOrTypes': ['Chunk'], 'properties': ['chunkId'], 'indexProvider': 'range-1.0', 'owningConstraint': 'unique_chunk', 'lastRead': None, 'readCount': None}]\n"
     ]
    }
   ],
   "source": [
    "# Create a uniqueness constraint on the chunkId property of Chunk nodes \n",
    "kg.query(\"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "\"\"\")\n",
    "\n",
    "created_indexes = kg.query('SHOW INDEXES')\n",
    "print(created_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all form10 files\n",
    "\n",
    "Perform the node creation for all files in an import directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create nodes for all chunks\n",
    "def create_nodes_for_all_chunks(chunks_with_metadata_list):\n",
    "    node_count = 0\n",
    "    for chunk in chunks_with_metadata_list:\n",
    "        print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkId']}\")\n",
    "        kg.query(merge_chunk_node_query, \n",
    "                params={\n",
    "                    'chunkParam': chunk\n",
    "                })\n",
    "        node_count += 1\n",
    "    print(f\"Created {node_count} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing 1 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0001650372-23-000040.json\n",
      "\tSplit into 4 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001650372-23-000040-item7a-chunk0003\n",
      "Created 64 nodes\n",
      "Done Processing ../data/sample/form10k/0001650372-23-000040.json\n",
      "=== Processing 2 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0000950170-23-033201.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0000950170-23-033201.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0000950170-23-033201.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0000950170-23-033201.json\n",
      "\tSplit into 3 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-033201-item7a-chunk0002\n",
      "Created 63 nodes\n",
      "Done Processing ../data/sample/form10k/0000950170-23-033201.json\n",
      "=== Processing 3 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0001564708-23-000368.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0001564708-23-000368.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0001564708-23-000368.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0001564708-23-000368.json\n",
      "\tSplit into 6 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001564708-23-000368-item7a-chunk0005\n",
      "Created 66 nodes\n",
      "Done Processing ../data/sample/form10k/0001564708-23-000368.json\n",
      "=== Processing 4 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0001558370-23-011516.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0001558370-23-011516.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0001558370-23-011516.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0001558370-23-011516.json\n",
      "\tSplit into 2 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001558370-23-011516-item7a-chunk0001\n",
      "Created 62 nodes\n",
      "Done Processing ../data/sample/form10k/0001558370-23-011516.json\n",
      "=== Processing 5 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0001096906-23-001489.json\n",
      "\tSplit into 3 chunks\n",
      "Processing item1a from ../data/sample/form10k/0001096906-23-001489.json\n",
      "\tSplit into 1 chunks\n",
      "Processing item7 from ../data/sample/form10k/0001096906-23-001489.json\n",
      "\tSplit into 7 chunks\n",
      "Processing item7a from ../data/sample/form10k/0001096906-23-001489.json\n",
      "\tSplit into 1 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001096906-23-001489-item7a-chunk0000\n",
      "Created 12 nodes\n",
      "Done Processing ../data/sample/form10k/0001096906-23-001489.json\n",
      "=== Processing 6 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0000950170-23-027948.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0000950170-23-027948.json\n",
      "\tSplit into 1 chunks\n",
      "Processing item7 from ../data/sample/form10k/0000950170-23-027948.json\n",
      "\tSplit into 1 chunks\n",
      "Processing item7a from ../data/sample/form10k/0000950170-23-027948.json\n",
      "\tSplit into 1 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item7a-chunk0000\n",
      "Created 23 nodes\n",
      "Done Processing ../data/sample/form10k/0000950170-23-027948.json\n",
      "=== Processing 7 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0001327567-23-000024.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0001327567-23-000024.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0001327567-23-000024.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0001327567-23-000024.json\n",
      "\tSplit into 3 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001327567-23-000024-item7a-chunk0002\n",
      "Created 63 nodes\n",
      "Done Processing ../data/sample/form10k/0001327567-23-000024.json\n",
      "=== Processing 8 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0000106040-23-000024.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0000106040-23-000024.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0000106040-23-000024.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0000106040-23-000024.json\n",
      "\tSplit into 3 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000106040-23-000024-item7a-chunk0002\n",
      "Created 63 nodes\n",
      "Done Processing ../data/sample/form10k/0000106040-23-000024.json\n",
      "=== Processing 9 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0000320187-23-000039.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0000320187-23-000039.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0000320187-23-000039.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0000320187-23-000039.json\n",
      "\tSplit into 4 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000320187-23-000039-item7a-chunk0003\n",
      "Created 64 nodes\n",
      "Done Processing ../data/sample/form10k/0000320187-23-000039.json\n",
      "=== Processing 10 of 10 ===\n",
      "Reading and splitting Form10k file...\n",
      "Processing item1 from ../data/sample/form10k/0001137789-23-000049.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ../data/sample/form10k/0001137789-23-000049.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7 from ../data/sample/form10k/0001137789-23-000049.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item7a from ../data/sample/form10k/0001137789-23-000049.json\n",
      "\tSplit into 4 chunks\n",
      "Creating Chunk Nodes...\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item1a-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7a-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7a-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0001137789-23-000049-item7a-chunk0003\n",
      "Created 64 nodes\n",
      "Done Processing ../data/sample/form10k/0001137789-23-000049.json\n",
      "CPU times: user 520 ms, sys: 74 ms, total: 594 ms\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'chunkCount': 544}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "IMPORT_DATA_DIRECTORY = f\"{DATA_DIR}/form10k/\"\n",
    "\n",
    "all_file_names = [IMPORT_DATA_DIRECTORY + x for x in os.listdir(IMPORT_DATA_DIRECTORY)]\n",
    "counter = 0\n",
    "\n",
    "for file_name in all_file_names:\n",
    "    counter += 1\n",
    "    print(f'=== Processing {counter} of {len(all_file_names)} ===')\n",
    "    # get and split text data\n",
    "    print('Reading and splitting Form10k file...')\n",
    "    chunk_list = split_form10k_data_from_file(file_name)\n",
    "    #load nodes\n",
    "    print('Creating Chunk Nodes...')\n",
    "    create_nodes_for_all_chunks(chunk_list)\n",
    "    print(f'Done Processing {file_name}')\n",
    "\n",
    "# Check the number of nodes in the graph\n",
    "kg.query(\"MATCH (n:Chunk) RETURN count(n) as chunkCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uniqueFormCount': 10}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"MATCH (c:Chunk) RETURN count(distinct(c.formId)) as uniqueFormCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uniqueCompanyCount': 10}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"MATCH (c:Chunk) RETURN count(distinct(c.cusip6)) as uniqueCompanyCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'c.names': ['ATLASSIAN CORP PLC', 'ATLASSIAN CORPORATION PLC']},\n",
       " {'c.names': ['FedEx Corp', 'FEDEX CORP']},\n",
       " {'c.names': ['NEWS CORP   CLASS B', 'News Corp.', 'NEWS CORP NEW']},\n",
       " {'c.names': ['GSI TECHNOLOGY INC']},\n",
       " {'c.names': ['APPLE INC']},\n",
       " {'c.names': ['Netapp Inc', 'NETAPP INC']},\n",
       " {'c.names': ['Palo Alto Networks Inc.',\n",
       "   'PALO ALTO NETWORKS INC',\n",
       "   'PALO ALTO NETWORKS INC PUT',\n",
       "   'None']},\n",
       " {'c.names': ['WESTERN DIGITAL CORP', 'WESTERN DIGITAL CORP.']},\n",
       " {'c.names': ['NIKE Inc.', 'NIKE INC']},\n",
       " {'c.names': ['SEAGATE TECHNOLOGY']}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"MATCH (c:Chunk) RETURN DISTINCT c.names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance - vector embeddings for the text of each chunk  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - prepare a vector index\n",
    "\n",
    "Now that you have a graph populated with `Chunk` nodes, \n",
    "you can add vector embeddings.\n",
    "\n",
    "First, prepare a vector index to store the embeddings.\n",
    "\n",
    "The index will be called `form_10k_chunks` and will store\n",
    "embeddings for nodes labeled as `Chunk` in a property\n",
    "called `textEmbedding`.\n",
    "\n",
    "The embeddings will match the recommended configuration\n",
    "for the OpenAI default embeddings model,\n",
    "with a dimension of 1,536\n",
    "and using the cosine similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector index called \"form_10k_chunks\" the `textEmbedding`` property of nodes labeled `Chunk`. \n",
    "# neo4j_create_vector_index(kg, VECTOR_INDEX_NAME, 'Chunk', 'textEmbedding')\n",
    "kg.query(\"\"\"\n",
    "         CREATE VECTOR INDEX `form_10k_chunks` IF NOT EXISTS\n",
    "          FOR (c:Chunk) ON (c.textEmbedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 1536,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - check the indexes\n",
    "\n",
    "You can check that the index was created successfully\n",
    "using \"SHOW INDEXES\".\n",
    "\n",
    "There's the vector index we just created, \n",
    "along with the uniqueness constraint from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 10,\n",
       "  'name': 'form_10k_chunks',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'VECTOR',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['textEmbedding'],\n",
       "  'indexProvider': 'vector-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': None,\n",
       "  'readCount': None},\n",
       " {'id': 7,\n",
       "  'name': 'unique_chunk',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'RANGE',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['chunkId'],\n",
       "  'indexProvider': 'range-1.0',\n",
       "  'owningConstraint': 'unique_chunk',\n",
       "  'lastRead': neo4j.time.DateTime(2024, 2, 25, 20, 54, 43, 971000000, tzinfo=<UTC>),\n",
       "  'readCount': 1694}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query('SHOW INDEXES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - creating text embeddings\n",
    "\n",
    "You can now use a single query to match all chunks,\n",
    "then `WITH` the chunk call \"OpenAI\" to get an embedding,\n",
    "and finally `SET` the embedding on each nodes.\n",
    "\n",
    "This may take a minute to run, depending on network traffic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vector embeddings for all the Chunk text, in batches.\n",
    "# Use this for larger number of chunks so that the query\n",
    "# can be re-run without losing all progress\n",
    "kg.query(\"\"\"\n",
    "  MATCH (chunk:Chunk) WHERE chunk.textEmbedding IS NULL\n",
    "  CALL {\n",
    "    WITH chunk\n",
    "    WITH chunk, genai.vector.encode(chunk.text, \"OpenAI\", {token: $openAiApiKey}) AS vector\n",
    "    CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", vector)    \n",
    "  } IN TRANSACTIONS OF 10 ROWS\n",
    "  \"\"\", \n",
    "  params={\"openAiApiKey\":OPENAI_API_KEY} \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example queries - vector similarity search with Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - a helper function for vector search using Neo4j\n",
    "\n",
    "You can now create a helper function to perform vector search\n",
    "using Neo4j. The function will accept a text question,\n",
    "then submit that to Neo4j as a parameter in a Cypher query.\n",
    "\n",
    "The query also accepts parameters for the OpenAI key,\n",
    "the vector index name, and the number of results to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  vector_search_query = \"\"\"\n",
    "    WITH genai.vector.encode($question, \"OpenAI\", \n",
    "        {token: $openAiApiKey}) AS question_embedding\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, question_embedding) \n",
    "        YIELD node, score\n",
    "    RETURN score, node.text AS text\n",
    "  \"\"\"\n",
    "  similar = kg.query(vector_search_query, \n",
    "                     params={\n",
    "                      'question': question, \n",
    "                      'openAiApiKey':OPENAI_API_KEY,\n",
    "                      'index_name':VECTOR_INDEX_NAME, \n",
    "                      'top_k': 10})\n",
    "  return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - try Neo4j vector search helper\n",
    "\n",
    "You may recall that the form we've turned into a knowledge graph is\n",
    "from a company called \"Netapp\".\n",
    "\n",
    "You can try our vector search helper function to ask about Netapp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9356340169906616,\n",
       " 'text': '>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = neo4j_vector_search(\n",
    "    'In a single sentence, tell me about Netapp.'\n",
    ")\n",
    "search_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG With Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI integration for Langchain (probably a slide)\n",
    "\n",
    "Notice that we only performed vector search. So what we're getting\n",
    "back is the raw chunk text.\n",
    "\n",
    "If we want to create a chatbot that provides actual answers to\n",
    "a question, we can build a RAG system using Langchain.\n",
    "\n",
    "Let's take a look at how you'll do that. (note to editor: show a slide!)\n",
    "\n",
    "The basic RAG flow goes through these steps: (possible diagram)\n",
    "\n",
    "1. accept a question from the user\n",
    "2. perform a database query to find relevant text that may provide an answer\n",
    "3. package the original question plus the relevant text into a prompt\n",
    "4. pass the entire prompt to an LLM to produce an answer\n",
    "5. finally, return the LLM's answer to the user\n",
    "\n",
    "Langchain is a great framework for creating a complete RAG workflow.\n",
    "\n",
    "It has excellent integration with Neo4j. \n",
    "\n",
    "OK, let's get back to the notebook to try this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j Vector Store\n",
    "\n",
    "The easiest way to start using Neo4j with Langchain\n",
    "is with the `Neo4jVector` interface.\n",
    "\n",
    "This makes Neo4j look like a vector store.\n",
    "\n",
    "Under the hood, it will use the Cypher language\n",
    "for performing vector similarity searches.\n",
    "\n",
    "The configuration specifies a few important things:\n",
    "- use OpenAI for embeddings\n",
    "- how to connect to the Neo4j database\n",
    "- the name of the vector index to use\n",
    "- the label of the nodes to search\n",
    "- the property name of the text on those nodes\n",
    "- and, the property name of the embeddings on those nodes\n",
    "\n",
    "That vector store then gets converted into a retriever\n",
    "and finally added to a Question & answer chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a langchain vector store from the existing Neo4j knowledge graph.\n",
    "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = neo4j_vector_store.as_retriever()\n",
    "\n",
    "# Create a chatbot Question & Answer chain from the retriever\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0), chain_type=\"stuff\", retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script - pretty chain helper function\n",
    "\n",
    "The prettychain helper function calls the \n",
    "chain then formats the answer to make it more readable.\n",
    "\n",
    "by pulling out just the `answer` field then\n",
    "printing with `textwrap` to limit each line to 80 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper function to pretty print the chain's response\n",
    "def prettychain(question: str) -> str:\n",
    "    \"\"\"Pretty print the chain's response to a question\"\"\"\n",
    "    response = chain({\"question\": question},\n",
    "        return_only_outputs=True,)\n",
    "    print(textwrap.fill(response['answer'], 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask some questions\n",
    "\n",
    "Finally, you can use the Langchain chain, which combines the retriever\n",
    "and the vector store into a nice question and answer interface.\n",
    "\n",
    "You can see both the answer and the source that the answer came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetApp's primary business is enterprise storage and data management, cloud\n",
      "storage, and cloud operations.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"What is Netapp's primary business?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Netapp is headquartered in San Jose, California.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"Where is Netapp headquartered?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no information about Apple in the provided content.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"Briefly tell me about Apple.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FedEx Corporation provides customers and businesses worldwide with a broad\n",
      "portfolio of transportation, e-commerce, and business services, offering\n",
      "integrated business solutions through operating companies competing\n",
      "collectively, operating collaboratively, and innovating digitally as one FedEx.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"In a single sentence, tell me about Fedex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
